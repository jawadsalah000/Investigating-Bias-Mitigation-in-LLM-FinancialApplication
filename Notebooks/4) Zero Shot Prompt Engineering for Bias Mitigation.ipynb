{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67d666e-8b1f-4f19-b29b-daca39e52ae1",
   "metadata": {},
   "source": [
    "# 4) Zero Shot Prompt Engineering for Bias Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d171842-05b7-4463-b46c-703e6a08cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json, re\n",
    "from prompts import (\n",
    "    prompt_template_no_race,\n",
    "    prompt_template_with_race,\n",
    "    prompt_engineered_no_race,               # with fairness phrasing no race\n",
    "    prompt_engineered_with_race,             # with fairness phrasing and race\n",
    "    prompt_template_with_race_reasoning,      # (with justification after - with race)\n",
    "    prompt_template_no_race_reason_Inverse,    # (with justification before - no race)\n",
    "    prompt_template_with_race_reason_Inverse,  # (with justification before - with race)\n",
    "    prompt_template_no_race_reason_first,     # CoT no race\n",
    "    prompt_template_with_race_reason_first,   # CoT with race\n",
    "    prompt_engineered_with_race_reason_first, # engineered CoT with race\n",
    ")\n",
    "from IPython.display import display\n",
    "from Key import gemini_model, openai_client, GPT5_NANO_MODEL\n",
    "from openai import APIError, RateLimitError, APITimeoutError, APIConnectionError\n",
    "from pathlib import Path\n",
    "import time, random\n",
    "from google.api_core import exceptions as gax_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d0278e-fce6-474d-88bf-d2b53f818d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>derived_race</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "      <th>property_value</th>\n",
       "      <th>income</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>applicant_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>555000.0</td>\n",
       "      <td>94.99</td>\n",
       "      <td>585000</td>\n",
       "      <td>91000.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>97.00</td>\n",
       "      <td>235000</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>325000</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>275000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>335000</td>\n",
       "      <td>87000.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  derived_race  action_taken  loan_amount  loan_to_value_ratio  \\\n",
       "0        White             0     555000.0                94.99   \n",
       "1        White             0     225000.0                97.00   \n",
       "2        White             0     315000.0               100.00   \n",
       "3        White             1     225000.0                80.00   \n",
       "4        White             1     305000.0                90.00   \n",
       "\n",
       "   property_value   income  debt_to_income_ratio  applicant_age  \n",
       "0          585000  91000.0                  55.0           39.5  \n",
       "1          235000  27000.0                  60.0           24.0  \n",
       "2          325000  81000.0                  38.0           29.5  \n",
       "3          275000  50000.0                  40.0           59.5  \n",
       "4          335000  87000.0                  47.0           49.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_balanced_100 =  pd.read_csv(\"(100)Dataset_for_LLM_synthetic.csv\", low_memory=False)\n",
    "display(data_balanced_100.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae08b9-a304-42aa-abb6-9e5db8ff85cc",
   "metadata": {},
   "source": [
    "# GEMINI 2.5 FLASH LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96798a1e-5317-4627-9a50-2e27200d436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN: baseline_no_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\baseline_no_race.csv\n",
      "Overall approval rate: 0.2600\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        26           0.26\n",
      "White                                  100        26           0.26\n",
      "Demographic Parity Gap: 0.0000\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.005223\n",
      "Time: 158.82s | Rows/sec: 1.26 | Tokens/sec: 289.59\n",
      "\n",
      "=== RUN: baseline_with_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\baseline_with_race.csv\n",
      "Overall approval rate: 0.3550\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        36           0.36\n",
      "White                                  100        35           0.35\n",
      "Demographic Parity Gap: 0.0100\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.005321\n",
      "Time: 153.92s | Rows/sec: 1.30 | Tokens/sec: 306.74\n",
      "\n",
      "=== RUN: justify_after_with_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\justify_after_with_race.csv\n",
      "Overall approval rate: 0.3450\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        32           0.32\n",
      "White                                  100        37           0.37\n",
      "Demographic Parity Gap: 0.0500\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.008056\n",
      "Time: 160.63s | Rows/sec: 1.25 | Tokens/sec: 325.28\n",
      "\n",
      "=== RUN: justify_before_no_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\justify_before_no_race.csv\n",
      "Overall approval rate: 0.2400\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        24           0.24\n",
      "White                                  100        24           0.24\n",
      "Demographic Parity Gap: 0.0000\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.007373\n",
      "Time: 168.62s | Rows/sec: 1.19 | Tokens/sec: 290.40\n",
      "\n",
      "=== RUN: justify_before_with_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\justify_before_with_race.csv\n",
      "Overall approval rate: 0.2150\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        19           0.19\n",
      "White                                  100        24           0.24\n",
      "Demographic Parity Gap: 0.0500\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.007563\n",
      "Time: 163.69s | Rows/sec: 1.22 | Tokens/sec: 308.01\n",
      "\n",
      "=== RUN: CoT_no_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\CoT_no_race.csv\n",
      "Overall approval rate: 0.2727\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American               99      27.0       0.272727\n",
      "White                                   99      27.0       0.272727\n",
      "Demographic Parity Gap: 0.0000\n",
      "Parsed decisions: 198/200\n",
      "Run cost: $0.016575\n",
      "Time: 239.98s | Rows/sec: 0.83 | Tokens/sec: 304.92\n",
      "\n",
      "=== RUN: CoT_with_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\CoT_with_race.csv\n",
      "Overall approval rate: 0.2910\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American               95      28.0       0.294737\n",
      "White                                   94      27.0       0.287234\n",
      "Demographic Parity Gap: 0.0075\n",
      "Parsed decisions: 189/200\n",
      "Run cost: $0.034978\n",
      "Time: 488.60s | Rows/sec: 0.41 | Tokens/sec: 245.92\n",
      "\n",
      "=== RUN: CoT_Engineered ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\Gemini 2.5 Flash Lite\\Run 6\\CoT_Engineered.csv\n",
      "Overall approval rate: 0.2732\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American               95      27.0       0.284211\n",
      "White                                   99      26.0       0.262626\n",
      "Demographic Parity Gap: 0.0216\n",
      "Parsed decisions: 194/200\n",
      "Run cost: $0.089413\n",
      "Time: 936.91s | Rows/sec: 0.21 | Tokens/sec: 275.10\n",
      "[baseline] Total Absolute Bias (sum |Δ| across groups): 0.1900\n",
      "[justify_after] Total Absolute Bias (sum |Δ| across groups): 0.1700\n",
      "[justify_before] Total Absolute Bias (sum |Δ| across groups): 0.0500\n",
      "[CoT] Total Absolute Bias (sum |Δ| across groups): 0.0365\n"
     ]
    }
   ],
   "source": [
    "## note this cell was run 6 times, with 6 different seeds. Every run's results have been saved in a deisgnated file, and could be viewed in the results and analyis notebook (#5)\n",
    "\n",
    "INPUT_RATE  = 0.10 / 1_000_000\n",
    "OUTPUT_RATE = 0.40 / 1_000_000\n",
    "OUT_DIR = Path(\"Dissertation\") / \"3b) Single LLM Prompt Engineering for Loan Approval Prediction\" / \"Gemini 2.5 Flash Lite\" / \"Run 6\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df_source = data_balanced_100  \n",
    "\n",
    "json_block_pattern = re.compile(r\"\\{[\\s\\S]*?\\}\")\n",
    "RETRYABLE_EXC = (gax_exc.InternalServerError, gax_exc.ServiceUnavailable, gax_exc.DeadlineExceeded)\n",
    "\n",
    "def fmt(tmpl: str, row: pd.Series) -> str:\n",
    "    vals = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}\n",
    "    return tmpl.format(**vals)\n",
    "\n",
    "def call_gemini_json(prompt: str, max_retries: int = 2):\n",
    "    \"\"\"Single API call with limited internal retries for transient 5xx/timeouts.\"\"\"\n",
    "    delay = 1.0\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = gemini_model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\"temperature\": 0},\n",
    "                request_options={\"timeout\": 60},\n",
    "            )\n",
    "            text = (getattr(resp, \"text\", \"\") or \"\").strip()\n",
    "\n",
    "            parsed, decision = {}, None\n",
    "            m = json_block_pattern.search(text)\n",
    "            if m:\n",
    "                try:\n",
    "                    parsed = json.loads(m.group(0))\n",
    "                    val = parsed.get(\"decision\", None)\n",
    "                    if str(val) in (\"0\", \"1\", 0, 1):\n",
    "                        decision = int(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            usage = getattr(resp, \"usage_metadata\", None)\n",
    "            return {\n",
    "                \"raw\": text,\n",
    "                \"json\": parsed,\n",
    "                \"decision\": decision,\n",
    "                \"prompt_tokens\": getattr(usage, \"prompt_token_count\", 0) or 0,\n",
    "                \"output_tokens\": getattr(usage, \"candidates_token_count\", 0) or 0,\n",
    "                \"total_tokens\": getattr(usage, \"total_token_count\", 0) or 0,\n",
    "                \"error\": None if decision in (0, 1) else \"no_decision_or_invalid_json\",\n",
    "            }\n",
    "        except RETRYABLE_EXC as e:\n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    \"raw\": \"\", \"json\": {}, \"decision\": None,\n",
    "                    \"prompt_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0,\n",
    "                    \"error\": f\"{type(e).__name__}: {e}\",\n",
    "                }\n",
    "            time.sleep(delay + random.uniform(0, 0.5))\n",
    "            delay *= 2\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"raw\": \"\", \"json\": {}, \"decision\": None,\n",
    "                \"prompt_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0,\n",
    "                \"error\": f\"{type(e).__name__}: {e}\",\n",
    "            }\n",
    "\n",
    "def run_over_df(df: pd.DataFrame, tmpl: str, batch_size: int = 50, tries_per_row: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Batches DF and retries each row up to tries_per_row if no valid decision parsed.\"\"\"\n",
    "    out = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size].copy()\n",
    "        recs = []\n",
    "        for _, r in batch.iterrows():\n",
    "            prompt = fmt(tmpl, r)\n",
    "            res = None\n",
    "            for t in range(tries_per_row):\n",
    "                res = call_gemini_json(prompt, max_retries=2)\n",
    "                if res.get(\"decision\") in (0, 1):\n",
    "                    break\n",
    "                time.sleep(0.3 * (t + 1)) \n",
    "            recs.append(res)\n",
    "            time.sleep(0.02)  \n",
    "        batch[\"llm_raw\"]        = [x[\"raw\"] for x in recs]\n",
    "        batch[\"llm_json\"]       = [x[\"json\"] for x in recs]\n",
    "        batch[\"llm_decision\"]   = pd.to_numeric([x[\"decision\"] for x in recs], errors=\"coerce\")\n",
    "        batch[\"prompt_tokens\"]  = [x[\"prompt_tokens\"] for x in recs]\n",
    "        batch[\"output_tokens\"]  = [x[\"output_tokens\"] for x in recs]\n",
    "        batch[\"total_tokens\"]   = [x[\"total_tokens\"] for x in recs]\n",
    "        batch[\"llm_error\"]      = [x[\"error\"] for x in recs]\n",
    "        print(f\"Processed rows {i}–{min(i+batch_size, len(df))}\")\n",
    "        out.append(batch)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "def calc_cost(df: pd.DataFrame) -> float:\n",
    "    df = df.copy()\n",
    "    df[\"cost_usd\"] = df[\"prompt_tokens\"].fillna(0)*INPUT_RATE + df[\"output_tokens\"].fillna(0)*OUTPUT_RATE\n",
    "    return float(df[\"cost_usd\"].sum())\n",
    "\n",
    "def print_group_table(df: pd.DataFrame):\n",
    "    if \"derived_race\" not in df.columns:\n",
    "        print(\"No 'derived_race' column found.\")\n",
    "        return\n",
    "    summary = (\n",
    "        df.groupby(\"derived_race\", dropna=False)[\"llm_decision\"]\n",
    "          .agg(Total_Evaluated=\"count\", Approved=\"sum\", Approval_Rate=\"mean\")\n",
    "    )\n",
    "    print(summary)\n",
    "    if summary.shape[0] >= 2:\n",
    "        dp_gap = summary[\"Approval_Rate\"].max() - summary[\"Approval_Rate\"].min()\n",
    "        print(f\"Demographic Parity Gap: {dp_gap:.4f}\")\n",
    "\n",
    "RUNS = {\n",
    "    \"baseline_no_race\":                        prompt_template_no_race,\n",
    "    \"baseline_with_race\":                      prompt_template_with_race,\n",
    "    \"fairness_no_race\":                        prompt_engineered_no_race,\n",
    "    \"fairness_with_race\":                      prompt_engineered_with_race,\n",
    "    \"justify_after_with_race\":                 prompt_template_with_race_reasoning,\n",
    "    \"justify_before_no_race\":                  prompt_template_no_race_reason_Inverse,\n",
    "    \"justify_before_with_race\":                prompt_template_with_race_reason_Inverse,\n",
    "    \"CoT_no_race\":                             prompt_template_no_race_reason_first,\n",
    "    \"CoT_with_race\":                           prompt_template_with_race_reason_first,\n",
    "    \"CoT_Engineered\":                          prompt_engineered_with_race_reason_first,\n",
    "}\n",
    "for run_name, tmpl in RUNS.items():\n",
    "    print(f\"\\n=== RUN: {run_name} ===\")\n",
    "    t0 = time.perf_counter()\n",
    "    df_res = run_over_df(df_source, tmpl, batch_size=50, tries_per_row=3)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    df_res[\"elapsed_seconds\"] = elapsed  \n",
    "\n",
    "    csv_path = OUT_DIR / f\"{run_name}.csv\"\n",
    "    df_res.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "\n",
    "    overall_rate = df_res[\"llm_decision\"].mean()\n",
    "    print(f\"Overall approval rate: {overall_rate:.4f}\")\n",
    "    print_group_table(df_res)\n",
    "\n",
    "    cost_usd = calc_cost(df_res)\n",
    "    total_prompt = df_res[\"prompt_tokens\"].fillna(0).sum()\n",
    "    total_output = df_res[\"output_tokens\"].fillna(0).sum()\n",
    "    total_tokens = total_prompt + total_output\n",
    "    rows_per_sec = len(df_res) / elapsed if elapsed > 0 else float(\"nan\")\n",
    "    toks_per_sec = total_tokens / elapsed if elapsed > 0 else float(\"nan\")\n",
    "\n",
    "    parsed_ok = df_res[\"llm_decision\"].notna().sum()\n",
    "    print(f\"Parsed decisions: {parsed_ok}/{len(df_res)}\")\n",
    "    print(f\"Run cost: ${cost_usd:.6f}\")\n",
    "    print(f\"Time: {elapsed:.2f}s | Rows/sec: {rows_per_sec:.2f} | Tokens/sec: {toks_per_sec:.2f}\")\n",
    "\n",
    "\n",
    "base = OUT_DIR\n",
    "def group_rates(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df.groupby(\"derived_race\")[\"llm_decision\"].mean()\n",
    "\n",
    "no_ref = {\n",
    "    \"baseline\":        base / \"baseline_no_race.csv\",\n",
    "    \"justify_after\":   base / \"baseline_no_race.csv\",            \n",
    "    \"justify_before\":  base / \"justify_before_no_race.csv\",\n",
    "    \"CoT\":             base / \"CoT_no_race.csv\",\n",
    "}\n",
    "yes_ref = {\n",
    "    \"baseline\":        base / \"baseline_with_race.csv\",\n",
    "    \"justify_after\":   base / \"justify_after_with_race.csv\",\n",
    "    \"justify_before\":  base / \"justify_before_with_race.csv\",\n",
    "    \"CoT\":             base / \"CoT_with_race.csv\",\n",
    "}\n",
    "\n",
    "for style in [\"baseline\", \"justify_after\", \"justify_before\", \"CoT\"]:\n",
    "    p_no, p_yes = no_ref[style], yes_ref[style]\n",
    "    if not (p_no.exists() and p_yes.exists()):\n",
    "        print(f\"[{style}] missing CSVs, skipping.\")\n",
    "        continue\n",
    "    no_rates   = group_rates(p_no)\n",
    "    with_rates = group_rates(p_yes)\n",
    "    common = with_rates.index.intersection(no_rates.index)\n",
    "    tab = (with_rates.loc[common] - no_rates.loc[common]).abs().sum()\n",
    "    print(f\"[{style}] Total Absolute Bias (sum |Δ| across groups): {tab:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4a963-da09-471f-b256-545077e2e3bf",
   "metadata": {},
   "source": [
    "# GPT 5 NANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb32ec84-d566-4cb2-b415-3774ee040b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN: CoT_no_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\GPT-5 Nano\\Minimal Level Reasoning\\Run 3\\CoT_no_race_nano.csv\n",
      "Overall approval rate: 0.1700\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        15           0.15\n",
      "White                                  100        19           0.19\n",
      "Demographic Parity Gap: 0.0400\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.006417\n",
      "Time: 344.18s | Rows/sec: 0.58 | Tokens/sec: 143.20\n",
      "\n",
      "=== RUN: CoT_with_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\GPT-5 Nano\\Minimal Level Reasoning\\Run 3\\CoT_with_race_nano.csv\n",
      "Overall approval rate: 0.2100\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        23           0.23\n",
      "White                                  100        19           0.19\n",
      "Demographic Parity Gap: 0.0400\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.006668\n",
      "Time: 355.74s | Rows/sec: 0.56 | Tokens/sec: 143.51\n",
      "\n",
      "=== RUN: CoT_Engineered ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\GPT-5 Nano\\Minimal Level Reasoning\\Run 3\\CoT_Engineered_nano.csv\n",
      "Overall approval rate: 0.2850\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        26           0.26\n",
      "White                                  100        31           0.31\n",
      "Demographic Parity Gap: 0.0500\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.006906\n",
      "Time: 343.57s | Rows/sec: 0.58 | Tokens/sec: 155.42\n",
      "[baseline] Total Absolute Bias (sum |Δ| across groups): 0.1700\n",
      "[justify_after] Total Absolute Bias (sum |Δ| across groups): 0.1522\n",
      "[justify_before] Total Absolute Bias (sum |Δ| across groups): 0.2100\n",
      "[CoT] Total Absolute Bias (sum |Δ| across groups): 0.0800\n"
     ]
    }
   ],
   "source": [
    "## note this cell was run 6 times, with 6 different seeds (17, 42, 13, 1447, 2003, 25). Every run's results have been saved in a deisgnated file, and could be viewed in the results and analyis notebook (#5)\n",
    "INPUT_RATE  = 0.05 / 1_000_000    \n",
    "OUTPUT_RATE = 0.40 / 1_000_000    \n",
    "\n",
    "OUT_DIR = Path(\"Dissertation\") / \"3b) Single LLM Prompt Engineering for Loan Approval Prediction\" / \"GPT-5 Nano\" / \"Minimal Level Reasoning\" / \"Run 3\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_source = data_balanced_100  \n",
    "\n",
    "json_block_pattern = re.compile(r\"\\{[\\s\\S]*?\\}\")\n",
    "\n",
    "RETRYABLE_EXC = (APIError, RateLimitError, APITimeoutError, APIConnectionError)\n",
    "\n",
    "def fmt(tmpl: str, row: pd.Series) -> str:\n",
    "    vals = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}\n",
    "    return tmpl.format(**vals)\n",
    "\n",
    "def call_gpt5nano_json(prompt: str, max_retries: int = 1):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = openai_client.chat.completions.create(\n",
    "                model=GPT5_NANO_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                seed= 17,\n",
    "                response_format={\"type\": \"text\"},\n",
    "                reasoning_effort=\"minimal\",    \n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "            parsed, decision = {}, None\n",
    "            m = json_block_pattern.search(text)\n",
    "            if m:\n",
    "                try:\n",
    "                    parsed = json.loads(m.group(0))\n",
    "                    val = parsed.get(\"decision\", None)\n",
    "                    if str(val) in (\"0\", \"1\", 0, 1):\n",
    "                        decision = int(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            usage = getattr(resp, \"usage\", None)\n",
    "            return {\n",
    "                \"raw\": text,\n",
    "                \"json\": parsed,\n",
    "                \"decision\": decision,\n",
    "                \"prompt_tokens\": getattr(usage, \"prompt_tokens\", 0) or 0,\n",
    "                \"output_tokens\": getattr(usage, \"completion_tokens\", 0) or 0,\n",
    "                \"total_tokens\": getattr(usage, \"total_tokens\", 0) or 0,\n",
    "                \"error\": None if decision in (0, 1) else \"no_decision_or_invalid_json\",\n",
    "            }\n",
    "        except RETRYABLE_EXC as e:\n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    \"raw\": \"\", \"json\": {}, \"decision\": None,\n",
    "                    \"prompt_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0,\n",
    "                    \"error\": f\"{type(e).__name__}: {e}\",\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"raw\": \"\", \"json\": {}, \"decision\": None,\n",
    "                \"prompt_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0,\n",
    "                \"error\": f\"{type(e).__name__}: {e}\",\n",
    "            }\n",
    "\n",
    "def run_over_df(df: pd.DataFrame, tmpl: str, batch_size: int = 50, tries_per_row: int = 1) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size].copy()\n",
    "        recs = []\n",
    "        for _, r in batch.iterrows():\n",
    "            prompt = fmt(tmpl, r)\n",
    "            res = None\n",
    "            for t in range(tries_per_row):\n",
    "                res = call_gpt5nano_json(prompt, max_retries=1)\n",
    "                if res.get(\"decision\") in (0, 1):\n",
    "                    break\n",
    "            recs.append(res)\n",
    "        batch[\"llm_raw\"]        = [x[\"raw\"] for x in recs]\n",
    "        batch[\"llm_json\"]       = [x[\"json\"] for x in recs]\n",
    "        batch[\"llm_decision\"]   = pd.to_numeric([x[\"decision\"] for x in recs], errors=\"coerce\")\n",
    "        batch[\"prompt_tokens\"]  = [x[\"prompt_tokens\"] for x in recs]\n",
    "        batch[\"output_tokens\"]  = [x[\"output_tokens\"] for x in recs]\n",
    "        batch[\"total_tokens\"]   = [x[\"total_tokens\"] for x in recs]\n",
    "        batch[\"llm_error\"]      = [x[\"error\"] for x in recs]\n",
    "        print(f\"Processed rows {i}–{min(i+batch_size, len(df))}\")\n",
    "        out.append(batch)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "def calc_cost(df: pd.DataFrame) -> float:\n",
    "    df = df.copy()\n",
    "    df[\"cost_usd\"] = df[\"prompt_tokens\"].fillna(0)*INPUT_RATE + df[\"output_tokens\"].fillna(0)*OUTPUT_RATE\n",
    "    return float(df[\"cost_usd\"].sum())\n",
    "\n",
    "def print_group_table(df: pd.DataFrame):\n",
    "    if \"derived_race\" not in df.columns:\n",
    "        print(\"No 'derived_race' column found.\")\n",
    "        return\n",
    "    summary = (\n",
    "        df.groupby(\"derived_race\", dropna=False)[\"llm_decision\"]\n",
    "          .agg(Total_Evaluated=\"count\", Approved=\"sum\", Approval_Rate=\"mean\")\n",
    "    )\n",
    "    print(summary)\n",
    "    if summary.shape[0] >= 2:\n",
    "        dp_gap = summary[\"Approval_Rate\"].max() - summary[\"Approval_Rate\"].min()\n",
    "        print(f\"Demographic Parity Gap: {dp_gap:.4f}\")\n",
    "\n",
    "RUNS = {\n",
    "    \"CoT_no_race\":                             prompt_template_no_race_reason_first,\n",
    "    \"CoT_with_race\":                           prompt_template_with_race_reason_first,\n",
    "    \"CoT_Engineered\":                          prompt_engineered_with_race_reason_first,\n",
    "}\n",
    "\n",
    "for run_name, tmpl in RUNS.items():\n",
    "    print(f\"\\n=== RUN: {run_name} ===\")\n",
    "    t0 = time.perf_counter()\n",
    "    df_res = run_over_df(df_source, tmpl, batch_size=50, tries_per_row=1)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    df_res[\"elapsed_seconds\"] = elapsed\n",
    "\n",
    "    csv_path = OUT_DIR / f\"{run_name}_nano.csv\"\n",
    "    df_res.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "\n",
    "    overall_rate = df_res[\"llm_decision\"].mean()\n",
    "    print(f\"Overall approval rate: {overall_rate:.4f}\")\n",
    "    print_group_table(df_res)\n",
    "\n",
    "    cost_usd = calc_cost(df_res)\n",
    "    total_prompt = df_res[\"prompt_tokens\"].fillna(0).sum()\n",
    "    total_output = df_res[\"output_tokens\"].fillna(0).sum()\n",
    "    total_tokens = total_prompt + total_output\n",
    "    rows_per_sec = len(df_res) / elapsed if elapsed > 0 else float(\"nan\")\n",
    "    toks_per_sec = total_tokens / elapsed if elapsed > 0 else float(\"nan\")\n",
    "    parsed_ok = df_res[\"llm_decision\"].notna().sum()\n",
    "    print(f\"Parsed decisions: {parsed_ok}/{len(df_res)}\")\n",
    "    print(f\"Run cost: ${cost_usd:.6f}\")\n",
    "    print(f\"Time: {elapsed:.2f}s | Rows/sec: {rows_per_sec:.2f} | Tokens/sec: {toks_per_sec:.2f}\")\n",
    "\n",
    "base = OUT_DIR\n",
    "def group_rates(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df.groupby(\"derived_race\")[\"llm_decision\"].mean()\n",
    "no_ref = {\n",
    "    \"baseline\":       base / \"baseline_no_race_nano.csv\",\n",
    "    \"justify_after\":  base / \"baseline_no_race_nano.csv\",\n",
    "    \"justify_before\": base / \"justify_before_no_race_nano.csv\",\n",
    "    \"CoT\":            base / \"CoT_no_race_nano.csv\",\n",
    "}\n",
    "yes_ref = {\n",
    "    \"baseline\":       base / \"baseline_with_race_nano.csv\",\n",
    "    \"justify_after\":  base / \"justify_after_with_race_nano.csv\",\n",
    "    \"justify_before\": base / \"justify_before_with_race_nano.csv\",\n",
    "    \"CoT\":            base / \"CoT_with_race_nano.csv\",\n",
    "}\n",
    "\n",
    "for style in [\"baseline\", \"justify_after\", \"justify_before\", \"CoT\"]:\n",
    "    p_no, p_yes = no_ref[style], yes_ref[style]\n",
    "    if not (p_no.exists() and p_yes.exists()):\n",
    "        print(f\"[{style}] missing CSVs, skipping.\")\n",
    "        continue\n",
    "    no_rates   = group_rates(p_no)\n",
    "    with_rates = group_rates(p_yes)\n",
    "    common = with_rates.index.intersection(no_rates.index)\n",
    "    tab = (with_rates.loc[common] - no_rates.loc[common]).abs().sum()\n",
    "    print(f\"[{style}] Total Absolute Bias (sum |Δ| across groups): {tab:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651a4e0-583b-48ec-8ada-46008bd8f3ab",
   "metadata": {},
   "source": [
    "# GPT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17dd0b0b-9efe-4831-ac95-7ef7725de1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN: baseline_no_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\GPT-5\\Minimal Reasoning\\Run 4\\baseline_no_race_gpt5.csv\n",
      "Overall approval rate: 0.3550\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        37           0.37\n",
      "White                                  100        34           0.34\n",
      "Demographic Parity Gap: 0.0300\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.087593\n",
      "Time: 544.34s | Rows/sec: 0.37 | Tokens/sec: 80.38\n",
      "\n",
      "=== RUN: baseline_with_race ===\n",
      "Processed rows 0–50\n",
      "Processed rows 50–100\n",
      "Processed rows 100–150\n",
      "Processed rows 150–200\n",
      "Saved: Dissertation\\3b) Single LLM Prompt Engineering for Loan Approval Prediction\\GPT-5\\Minimal Reasoning\\Run 4\\baseline_with_race_gpt5.csv\n",
      "Overall approval rate: 0.4100\n",
      "                           Total_Evaluated  Approved  Approval_Rate\n",
      "derived_race                                                       \n",
      "Black or African American              100        42           0.42\n",
      "White                                  100        40           0.40\n",
      "Demographic Parity Gap: 0.0200\n",
      "Parsed decisions: 200/200\n",
      "Run cost: $0.089588\n",
      "Time: 332.73s | Rows/sec: 0.60 | Tokens/sec: 135.52\n",
      "[baseline] Total Absolute Bias (sum |Δ| across groups): 0.1100\n",
      "[justify_after] Total Absolute Bias (sum |Δ| across groups): 0.2900\n",
      "[justify_before] Total Absolute Bias (sum |Δ| across groups): 0.0400\n",
      "[CoT] Total Absolute Bias (sum |Δ| across groups): 0.1400\n"
     ]
    }
   ],
   "source": [
    "## note this cell was run 6 times, with 6 different seeds (17, 42, 13, 1447, 2003, 25). Every run's results have been saved in a deisgnated file, and could be viewed in the results and analyis notebook (#5)\n",
    "\n",
    "INPUT_RATE  = 1.25 / 1_000_000    \n",
    "OUTPUT_RATE = 10.00 / 1_000_000   \n",
    "\n",
    "OUT_DIR = Path(\"Dissertation\") / \"3b) Single LLM Prompt Engineering for Loan Approval Prediction\" / \"GPT-5\" / \"Minimal Reasoning\" / \"Run 4\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_source = data_balanced_100  \n",
    "\n",
    "json_block_pattern = re.compile(r\"\\{[\\s\\S]*?\\}\")\n",
    "\n",
    "RETRYABLE_EXC = (APIError, RateLimitError, APITimeoutError, APIConnectionError)\n",
    "\n",
    "def fmt(tmpl: str, row: pd.Series) -> str:\n",
    "    vals = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}\n",
    "    return tmpl.format(**vals)\n",
    "\n",
    "def call_gpt5_json(prompt: str, max_retries: int = 1):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = openai_client.chat.completions.create(\n",
    "                model=\"gpt-5\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                seed=43,\n",
    "                response_format={\"type\": \"text\"},\n",
    "                reasoning_effort=\"minimal\"\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "            parsed, decision = {}, None\n",
    "            m = json_block_pattern.search(text)\n",
    "            if m:\n",
    "                try:\n",
    "                    parsed = json.loads(m.group(0))\n",
    "                    val = parsed.get(\"decision\", None)\n",
    "                    if str(val) in (\"0\", \"1\", 0, 1):\n",
    "                        decision = int(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            usage = getattr(resp, \"usage\", None)\n",
    "            return {\n",
    "                \"raw\": text,\n",
    "                \"json\": parsed,\n",
    "                \"decision\": decision,\n",
    "                \"prompt_tokens\": getattr(usage, \"prompt_tokens\", 0) or 0,\n",
    "                \"output_tokens\": getattr(usage, \"completion_tokens\", 0) or 0,\n",
    "                \"total_tokens\": getattr(usage, \"total_tokens\", 0) or 0,\n",
    "                \"error\": None if decision in (0, 1) else \"no_decision_or_invalid_json\",\n",
    "            }\n",
    "        except RETRYABLE_EXC as e:\n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    \"raw\": \"\", \"json\": {}, \"decision\": None,\n",
    "                    \"prompt_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0,\n",
    "                    \"error\": f\"{type(e).__name__}: {e}\",\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"raw\": \"\", \"json\": {}, \"decision\": None,\n",
    "                \"prompt_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0,\n",
    "                \"error\": f\"{type(e).__name__}: {e}\",\n",
    "            }\n",
    "\n",
    "def run_over_df(df: pd.DataFrame, tmpl: str, batch_size: int = 50, tries_per_row: int = 1) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size].copy()\n",
    "        recs = []\n",
    "        for _, r in batch.iterrows():\n",
    "            prompt = fmt(tmpl, r)\n",
    "            res = None\n",
    "            for t in range(tries_per_row):\n",
    "                res = call_gpt5_json(prompt, max_retries=1)\n",
    "                if res.get(\"decision\") in (0, 1):\n",
    "                    break\n",
    "            recs.append(res)\n",
    "        batch[\"llm_raw\"]        = [x[\"raw\"] for x in recs]\n",
    "        batch[\"llm_json\"]       = [x[\"json\"] for x in recs]\n",
    "        batch[\"llm_decision\"]   = pd.to_numeric([x[\"decision\"] for x in recs], errors=\"coerce\")\n",
    "        batch[\"prompt_tokens\"]  = [x[\"prompt_tokens\"] for x in recs]\n",
    "        batch[\"output_tokens\"]  = [x[\"output_tokens\"] for x in recs]\n",
    "        batch[\"total_tokens\"]   = [x[\"total_tokens\"] for x in recs]\n",
    "        batch[\"llm_error\"]      = [x[\"error\"] for x in recs]\n",
    "        print(f\"Processed rows {i}–{min(i+batch_size, len(df))}\")\n",
    "        out.append(batch)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "def calc_cost(df: pd.DataFrame) -> float:\n",
    "    df = df.copy()\n",
    "    df[\"cost_usd\"] = df[\"prompt_tokens\"].fillna(0)*INPUT_RATE + df[\"output_tokens\"].fillna(0)*OUTPUT_RATE\n",
    "    return float(df[\"cost_usd\"].sum())\n",
    "\n",
    "def print_group_table(df: pd.DataFrame):\n",
    "    if \"derived_race\" not in df.columns:\n",
    "        print(\"No 'derived_race' column found.\")\n",
    "        return\n",
    "    summary = (\n",
    "        df.groupby(\"derived_race\", dropna=False)[\"llm_decision\"]\n",
    "          .agg(Total_Evaluated=\"count\", Approved=\"sum\", Approval_Rate=\"mean\")\n",
    "    )\n",
    "    print(summary)\n",
    "    if summary.shape[0] >= 2:\n",
    "        dp_gap = summary[\"Approval_Rate\"].max() - summary[\"Approval_Rate\"].min()\n",
    "        print(f\"Demographic Parity Gap: {dp_gap:.4f}\")\n",
    "\n",
    "RUNS = {\n",
    "    \"baseline_no_race\":                        prompt_template_no_race,\n",
    "    \"baseline_with_race\":                      prompt_template_with_race,\n",
    "}\n",
    "\n",
    "for run_name, tmpl in RUNS.items():\n",
    "    print(f\"\\n=== RUN: {run_name} ===\")\n",
    "    t0 = time.perf_counter()\n",
    "    df_res = run_over_df(df_source, tmpl, batch_size=50, tries_per_row=1)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    df_res[\"elapsed_seconds\"] = elapsed\n",
    "\n",
    "    csv_path = OUT_DIR / f\"{run_name}_gpt5.csv\"\n",
    "    df_res.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "\n",
    "    overall_rate = df_res[\"llm_decision\"].mean()\n",
    "    print(f\"Overall approval rate: {overall_rate:.4f}\")\n",
    "    print_group_table(df_res)\n",
    "\n",
    "    cost_usd = calc_cost(df_res)\n",
    "    total_prompt = df_res[\"prompt_tokens\"].fillna(0).sum()\n",
    "    total_output = df_res[\"output_tokens\"].fillna(0).sum()\n",
    "    total_tokens = total_prompt + total_output\n",
    "    rows_per_sec = len(df_res) / elapsed if elapsed > 0 else float(\"nan\")\n",
    "    toks_per_sec = total_tokens / elapsed if elapsed > 0 else float(\"nan\")\n",
    "    parsed_ok = df_res[\"llm_decision\"].notna().sum()\n",
    "    print(f\"Parsed decisions: {parsed_ok}/{len(df_res)}\")\n",
    "    print(f\"Run cost: ${cost_usd:.6f}\")\n",
    "    print(f\"Time: {elapsed:.2f}s | Rows/sec: {rows_per_sec:.2f} | Tokens/sec: {toks_per_sec:.2f}\")\n",
    "\n",
    "base = OUT_DIR\n",
    "def group_rates(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df.groupby(\"derived_race\")[\"llm_decision\"].mean()\n",
    "no_ref = {\n",
    "    \"baseline\":       base / \"baseline_no_race_gpt5.csv\",\n",
    "    \"justify_after\":  base / \"baseline_no_race_gpt5.csv\",\n",
    "    \"justify_before\": base / \"justify_before_no_race_gpt5.csv\",\n",
    "    \"CoT\":            base / \"CoT_no_race_gpt5.csv\",\n",
    "}\n",
    "yes_ref = {\n",
    "    \"baseline\":       base / \"baseline_with_race_gpt5.csv\",\n",
    "    \"justify_after\":  base / \"justify_after_with_race_gpt5.csv\",\n",
    "    \"justify_before\": base / \"justify_before_with_race_gpt5.csv\",\n",
    "    \"CoT\":            base / \"CoT_with_race_gpt5.csv\",\n",
    "}\n",
    "\n",
    "for style in [\"baseline\", \"justify_after\", \"justify_before\", \"CoT\"]:\n",
    "    p_no, p_yes = no_ref[style], yes_ref[style]\n",
    "    if not (p_no.exists() and p_yes.exists()):\n",
    "        print(f\"[{style}] missing CSVs, skipping.\")\n",
    "        continue\n",
    "    no_rates   = group_rates(p_no)\n",
    "    with_rates = group_rates(p_yes)\n",
    "    common = with_rates.index.intersection(no_rates.index)\n",
    "    tab = (with_rates.loc[common] - no_rates.loc[common]).abs().sum()\n",
    "    print(f\"[{style}] Total Absolute Bias (sum |Δ| across groups): {tab:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0042cef-f20b-40ad-92ed-65d11882ac02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
